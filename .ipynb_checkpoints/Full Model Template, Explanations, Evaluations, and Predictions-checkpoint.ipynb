{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Articial Intelligence and Machine Learning - Coursework 1 - 1st diet\n",
    "## Air quality dataset\n",
    "# Student Name: \n",
    "# Student Email:\n",
    "\n",
    "I confirm that the material contained within the submitted coursework is all my own work unless otherwise stated below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and problem definition\n",
    "\n",
    "The dataset shows different air pollutants and their hourly average concentrations within a significantly polluted city in Italy. Understanding the impact of these air pollutants can allow us make predictions of the air quality at a given point in time as well as predict the concentration of certain air pollutants like Carbon monoxide(CO). This project helps to draw insight into understanding the impact these pollutants affect the air we breathe, offer triggers at certain times when the concentration of air pollutants like Carbon Monoxide(CO) becomes harzadous in a region.\n",
    "\n",
    "\n",
    "UNDERSTANDING THE TASK\n",
    "- Task 1 ---> Predicting the CO concentration (in mg/m3): \n",
    "    For this regression analysis problem we want to be able to get the level of concentration of Carbon Monoxide in the air. This problem is defined as regressional given we aren't trying to predict categories and what we are trying to predict is continious. Using features we defined in our data preparation section, we are able to train this regression model to predict the levels of Carbon Monoxide concentrations.\n",
    "    \n",
    "- Task 2 ---> Define your own Air Quality Index:\n",
    "    Upon creation, our air quality index is a range of values between 0 to 4 that grade the level of air quality at a given point in time given the pollutants in the air. The least value 0 indicates good air quality and the best air conditions while 4 indicates a very unhealthy or harzardous air to breathe. Given we are trying to predict values within a category of 0 to 4, this defines our classification problem.\n",
    "    \n",
    "\n",
    "<br>DEFINING THE PROBLEM STATEMENT OF OUR PROJECT\n",
    "\n",
    ">Air pollution is a major environmental concern globally, with significant implications for human health. Carbon monoxide (CO) is a particularly harmful air pollutant, causing various adverse health effects, including headaches, dizziness, nausea, and even death in high concentrations.\n",
    ">This project aims to address the air pollution challenge in a heavily polluted city in Italy. We will focus on developing solutions for two specific tasks:\n",
    ">\n",
    ">**Task 1** ---> Predicting Carbon Monoxide (CO) Concentration:\n",
    ">1. Objective: Develop a robust model to predict the hourly average CO concentration in the city based on available environmental data.\n",
    ">2. Methodology: Employ machine learning regression techniques to analyze the dataset containing various air pollutant concentrations and meteorological factors.\n",
    ">3. Expected Outcome: A reliable model capable of predicting CO concentration with high accuracy, allowing for informed decision-making related to public health and air quality management.\n",
    ">\n",
    ">\n",
    ">**Task 2** ---> Defining an Air Quality Index (AQI):\n",
    ">1. Objective: Create a user-friendly AQI specifically tailored for the Italian city, providing a clear and concise assessment of overall air quality based on multiple pollutants.\n",
    ">2. Methodology: Analyze the relationships between individual pollutants and their combined impact on public health. Develop a scoring system that assigns a numerical value to different air quality levels, ranging from \"Good\" to \"Hazardous.\"\n",
    ">3. Expected Outcome: An AQI that effectively communicates the city's air quality to the public, enabling them to make informed decisions about their health and activities.\n",
    "\n",
    "This project's success is centered on our ability to develop a CO prediction model with high accuracy and generalizability. The ability to create an AQI that is easy to understand and interpret by the public. Utilize predictions and the AQI to inform air pollution management strategies and public health advisories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data ingeston\n",
    "\n",
    "We get our dataset into our notebook using the pandas read_csv function. Using the pandas info function, we get some information on the statistical data types of each column in our dataset. These are our findings:\n",
    "0.   Date ---> object \n",
    "1.   Time ---> object \n",
    "2.   CO(GT) ---> float64\n",
    "3.   PT08.S1(CO) ---> float64\n",
    "4.   NMHC(GT) ---> float64\n",
    "5.   C6H6(GT) ---> float64\n",
    "6.   PT08.S2(NMHC) ---> float64\n",
    "7.   NOx(GT) ---> float64\n",
    "8.   PT08.S3(NOx) ---> float64\n",
    "9.   NO2(GT) ---> float64\n",
    "10.  PT08.S4(NO2) ---> float64\n",
    "11.  PT08.S5(O3) ---> float64\n",
    "12.  T(C) ---> float64\n",
    "13.  RH ---> float64\n",
    "14.  AH ---> float64\n",
    "15.  DayOfWeekName ---> object\n",
    "16.  PeakTime ---> int64  \n",
    "17.  ValleyTime ---> int64  \n",
    "\n",
    "dtypes: float64(13), int64(2), object(3)\n",
    "\n",
    "\n",
    "#### Statistical Data Types:\n",
    "We have two types. The two data types are Numerical and Categorical.\n",
    "+ Numerical Data Types -----> float64 and int64\n",
    "+ Categorical Data Types -----> object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the dataset\n",
    "dataset = pd.read_csv(\"AirQuality.csv\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial descriptive statistics\n",
    "print(\"Data Schema\")\n",
    "dataset.info()\n",
    "\n",
    "dataset_descriptive_statistic = dataset.describe()\n",
    "print(f\"\\n\\nData descriptive statistics: \\n{dataset_descriptive_statistic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preparation (common to both tasks)\n",
    "\n",
    "Using the assumption that initial exploratory analysis has been done, our data preparation steps include the following:\n",
    "- Droping the empty columns, and any other column that we won’t need for this analysis\n",
    "- Replacing the “-200” values by np.nan for the correct operations of the usual functions\n",
    "- Creating a new attribute (column) indicating the day of the week, for instance using:\n",
    "df[\"DayOfWeek\"] = pd.to_datetime(df[\"Date\"], dayfirst=True).dt.day_name()\n",
    "- Creating a new field that indicates whether it is a peak time or not\n",
    "- Creating a new field that indicates whether it is valley time or not\n",
    "\n",
    "COLUMNS DROPPED:\n",
    "- Step 1: We started by dropping these columns ---> \"Unnamed: 15\" and \"Unnamed: 16\" as they are are completely NULL.\n",
    "- Step 2: We drop the NMHC air pollutant column. The data consists over 9800 rows. For the NMHC, over 8800 of this data is missing. Having more than 3/4 of this data missing, could lead to bias in our model if used as a model for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Dropping the empty columns\n",
    "data = dataset.drop([\"Unnamed: 15\", \"Unnamed: 16\"], axis = 1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Replacing all values with -200 with np.nan\n",
    "data[data == -200] = np.nan\n",
    "null_check = data.isnull().sum()\n",
    "print(data)\n",
    "print(f\"NULL COUNT: \\n\\n{null_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Creating the day of the week column\n",
    "data[\"DayOfWeek\"] = pd.to_datetime(data[\"Date\"], dayfirst = True).dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Creating a column indicating Peak Time or Not. \n",
    "# (8AM-12PM and 6-10PM on working days) and (9am-12pm on non-working days).\n",
    "def peak_time(dayofweek):\n",
    "    WorkingDays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
    "    NonWorkingDays = [\"Saturday\", \"Sunday\"]\n",
    "\n",
    "    if (dayofweek[\"DayOfWeek\"] in WorkingDays) and (dayofweek[\"Time\"] >= \"08:00:00\" and dayofweek[\"Time\"] <= \"12:00:00\"):\n",
    "        return 1\n",
    "    elif (dayofweek[\"DayOfWeek\"] in WorkingDays) and (dayofweek[\"Time\"] >= \"18:00:00\" and dayofweek[\"Time\"] <= \"22:00:00\"):\n",
    "        return 1\n",
    "    elif (dayofweek[\"DayOfWeek\"] in NonWorkingDays) and (dayofweek[\"Time\"] >= \"09:00:00\" and dayofweek[\"Time\"] <= \"12:00:00\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    " \n",
    "data[\"PeakTime\"] = data.apply(peak_time, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) Creating a column indicating Valley Road Usage or Not\n",
    "# (Valley road usage is during the central hours of the night (2-6am))\n",
    "def valley_time(time):\n",
    "    if time[\"Time\"] >= \"02:00:00\" and time[\"Time\"] <= \"06:00:00\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data[\"ValleyTime\"] = data.apply(valley_time, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) Exploratory Data Analysis\n",
    "data.info()\n",
    "data_head = data.head()\n",
    "print(f\"Data Head: \\n\\n{data_head}\")\n",
    "data_tail = data.tail()\n",
    "print(f\"Data Tail: \\n\\n{data_tail}\")\n",
    "data_descriptive_statistic = data.describe()\n",
    "print(f\"Descriptive Statistics: \\n\\n{data_descriptive_statistic}\")\n",
    "data_distinct_count = data.nunique()\n",
    "print(f\"Data Distinct Count: \\n\\n{data_distinct_count}\")\n",
    "data_correlation_matrix = data.corr() \n",
    "print(f\"Correlation Matrix: \\n\\n{data_correlation_matrix}\")\n",
    "data_null_count = data.isnull().sum()\n",
    "print(f\"Missing Values in each Column: \\n\\n{data_null_count}\")\n",
    "data_total_null_count = data.isnull().sum().sum()\n",
    "print(f\"Data Total Missing Values: {data_total_null_count}\")\n",
    "\n",
    "            # ---> Visualization\n",
    "data_histogram = data.hist(bins = 10, figsize = (30, 15), alpha=0.7, color='brown')\n",
    "plt.figure(figsize = (30, 10))\n",
    "data_heatmap = sns.heatmap(data_correlation_matrix, annot = True, cmap = \"coolwarm\")\n",
    "plt.title('Correlation Matrix of Independent Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) Creating a copy of the data after initial data preparation to allow us work on our seperate tasks\n",
    "data_task1 = data\n",
    "data_task2 = data\n",
    "print(data_task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (8) Dropping the NMHC columns readings due to excessive missing values in the columns\n",
    "data_task1 = data_task1.drop([\"NMHC(GT)\"], axis = 1)\n",
    "print(data_task1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1: CO concentration prediction\n",
    "Predict the CO concentration (in mg/m3) based on, at least, the PT08.S1(CO) raw sensor readings, day of the week and time. \n",
    "\n",
    "Maybe temperature and humidity can play a role as well? \n",
    "\n",
    "Use CO(GT) as the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Further Data preparation (specific for this task)\n",
    "## Data segregation\n",
    "\n",
    "In this section, we remove additional columns that we won't be needing for our model prediction. Removing all the ground truth labels except the CO(GT) which remains what we we predict. Using the raw sensor readings, time, and day of week as our base features, we train our model. \n",
    "\n",
    "DATA BINNING\n",
    "\n",
    "Our choice to not create bins or groupings of the numerical data for each columns was to avoid loss of information in prediction. However, we did use data binning to allow us be able to plot the proper distribution of the data in each columns which further allows us to understand whether we have skewed or normal distributions which in turn influences our choice of strategy for further handling missing values in our data. In our visualization, we use data binning to plot the graph of our Histogram which shows us the distribution of data in each column.\n",
    "\n",
    "\n",
    "FIXING MISSING VALUES\n",
    "\n",
    "The data we are working with after doing some prior data preparation, is still left with missing values in the columns, to fix this, we set our strategy towards using the median value of the distribution in each column to fix this. While performing some statistical analysis for each of the columns to understand it's distribution, we find out that all columns are not normally distributed asides from T(C), RH, and AH which are either normally distributed or close to normal. Given that the median, mean, and mode are all same or close to each other in a normal distribution, we set our strategy for fixing missing values to MEDIAN. This is the case given the median is a valid measure of center in skewed distributions and does a better job than the mean.\n",
    "\n",
    "\n",
    "DATA SPLITTING\n",
    "\n",
    "Using an 80:20 split ratio is industry standard and allows us to use 80 percent of our data for training while 20 percent accounts for testing the models accuracy after prediction. This 20 percent also serves as basis of what we use to judge and evaluate how good our model is before deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Dropping other ground truth readings\n",
    "data = data_task1.drop([\"C6H6(GT)\", \"NOx(GT)\", \"NO2(GT)\",], axis = 1)\n",
    "count_null = data.isnull().sum()\n",
    "print(data)\n",
    "print(f\"\\n\\n\\nMissing Values in Columns: \\n{count_null}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Dropping all missing values in our label CO(GT) to improve our prediction and allow us train the model om the True Labels\n",
    "data = data.dropna(subset = \"CO(GT)\")\n",
    "count_null = data.isnull().sum()\n",
    "print(data)\n",
    "print(f\"\\n\\n\\nMissing Values in Columns: \\n{count_null}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Fixing Missing Values\n",
    "impute = SimpleImputer(strategy = \"median\")\n",
    "data.iloc[:, [3, 4, 5, 6, 7, 8, 9, 10]] = impute.fit_transform(data.iloc[:, [3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "count_null = data.isnull().sum()\n",
    "print(data)\n",
    "print(f\"\\n\\n\\nMissing Values in Columns: \\n{count_null}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Extracting Features from Date and Time to Create New Features\n",
    "data['Datetime'] = data['Date'] + \" \" + data['Time']\n",
    "data.set_index(\"Datetime\", inplace = True)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = data.sort_index(axis = 0)\n",
    "\n",
    "data[\"Year\"] = data.index.year\n",
    "data[\"Month\"] = data.index.month\n",
    "data[\"Day\"] = data.index.day\n",
    "data[\"HourTime\"] = data.index.hour\n",
    "data[\"DayOfWeek\"] = data.index.day_of_week\n",
    "data[\"Quarter\"] = data.index.quarter\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) Dropping additional columns we won't be needing\n",
    "data = data.drop([\"Date\", 'Time'], axis = 1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) Grouping dependent and independent variables for prediction\n",
    "x = data.drop([\"CO(GT)\"], axis = 1) \n",
    "y = data[\"CO(GT)\"]\n",
    "print(f\"Independent Variables: \\n{x}\")\n",
    "print(f\"\\n\\nDependent Variables: \\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) Splitting the dataset (80:20)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "print(f\"x_train: \\n{x_train}\")\n",
    "print(f\"y_train: \\n{y_train}\")\n",
    "print(f\"x_test: \\n{x_test}\")\n",
    "print(f\"y_test: \\n{y_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model definition and training\n",
    "\n",
    "To train our model, we employ the popular Extreme Gradient Boosting algorithm (XGBoost). Using its regressor as the base model, the algorithm proves why it has become a go-to option for many machine learning and model creation solutions. The choice to use the XGBoost Regressor wasn't a difficult one. After running multiple simulations with other regression algorithms to train our model, including the LinearRegression model, SVR model, DecisionTreeRegressor model, RandomForestRegressor model, and more from sklearn, the XGBRegressor provided the best solution for training our model.\n",
    "\n",
    "MODEL OPTIMIZATION\n",
    "\n",
    "We tried optimizing our XGBoost regressor to see if we could improve the test R squared and further reduce the Root Mean Squared Error. We noticed slight improvements however, the changes are not significant enough to be considered a major upgrade from our baseline model.\n",
    "\n",
    "TRAINING AND PREDICTION\n",
    "\n",
    "While creating our model, we understand the need to focus on the test data as it is the basis of predictions and recommendations. However, we take into account the predictions and happenings while the model was being trained as well. This gives us a clearly image of the stages the model passed through and it's behaviour while training, before prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Base Model Training\n",
    "regressor = XGBRegressor()\n",
    "model = regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Base Model Prediction\n",
    "y_pred = model.predict(x_train)\n",
    "y_pred1 = model.predict(x_test)\n",
    "print(f\"Predictions from Training Data: \\n{y_pred}\")\n",
    "print(f\"Predictions from Test Data: \\n{y_pred1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIMIZED MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Optimized Model Training\n",
    "optimized_regressor = XGBRegressor(n_estimators=1000, learning_rate=0.1)\n",
    "optimized_model = optimized_regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Optimized Model Prediction\n",
    "optimized_y_pred = optimized_model.predict(x_train)\n",
    "optimized_y_pred1 = optimized_model.predict(x_test)\n",
    "print(f\"Predictions from Training Data: \\n{optimized_y_pred}\")\n",
    "print(f\"Predictions from Test Data: \\n{optimized_y_pred1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model evaluation\n",
    "\n",
    "To evaluate our regression model we have created, we will utilize two major metrics amongst others. These metrics are the Root Mean Squared Error(RMSE) and the R-Squared. These two metrics cover the foundations that explain how well our model has been trained and can make predictions. \n",
    "\n",
    "R-Squared ---> Also referred to as the Coefficient of Determination, measures what extent of the variation in our label Y is explained by the features we used to train our model. R-Squared ranges from 0 to 1 with 1 indicating that the features(x) explain perfectly, the variations in our label(Y) while a value of 0 indicates that the features have no way of correlating with the label and serve no purpose in explaining the variations in Y.\n",
    "\n",
    "RMSE ---> The root mean squared error, measures the extent of errors made in our model while making predictions. It checks the total errors across all data points between predictions and the actual values. With 0 indicating no errors in prediction, lower values indicate a better model at prediction. Higher values indicating more errors in prediction.\n",
    "\n",
    "CROSS VALIDATION ---> A technique popular in machine learning that allows you to bootstrap the dataset and run multiple series of simulations of training and testing on the dataset in order to get a clearer picture on the true accuracy in prediction that our model posseses. This step is considered very improtant in building any model.\n",
    "\n",
    "\n",
    "\n",
    "BASE MODEL vs OPTIMIZED MODEL\n",
    "- Our base model without any hyper parameter tuning has an r-squared of 91%, rmse of 0.42, a cross validation mean of 88% and cross validation standard deviation of 2.5 for our model. This is a good start given we are able to achieve this without any hyper parameter tuning.\n",
    "\n",
    "- Creating an optimized model allows us to test and see to what extent we can push the ability of our model to predict better than it's base form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Base Model Evaluation\n",
    "# Training Evaluation\n",
    "rmse_training = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "r2_training = r2_score(y_train, y_pred)\n",
    "print(f\"RMSE for Training Data: \\n{rmse_training}\")\n",
    "print(f\"R-Squared for Training Data: \\n{r2_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Evaluation\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred1))\n",
    "r2_test = r2_score(y_test, y_pred1)\n",
    "print(f\"RMSE for Test Data: \\n{rmse_test}\")\n",
    "print(f\"R-Squared for Test Data: \\n{r2_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Base Model Cross Validation\n",
    "score = cross_val_score(regressor, x_test, y_test, cv = 10)\n",
    "score_mean = round((score.mean() * 100), 2)\n",
    "score_std_dev = round((score.std() * 100), 2)\n",
    "print(f\"Cross Validation Mean: {score_mean}\")\n",
    "print(f\"Cross Validation Standard Deviation: {score_std_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Feature Importance\n",
    "imp_features = pd.DataFrame({\"Features\": model.feature_names_in_, \"Score\": model.feature_importances_})\n",
    "print(f\"Important Features from Training: \\n{imp_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIMIZED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Optimized Model Evaluation\n",
    "# Training Evaluation\n",
    "optimized_rmse_training = np.sqrt(mean_squared_error(y_train, optimized_y_pred))\n",
    "optimized_r2_training = np.sqrt(r2_score(y_train, optimized_y_pred))\n",
    "print(f\"RMSE for Training Data: \\n{optimized_rmse_training}\")\n",
    "print(f\"R-Squared for Training Data: \\n{optimized_r2_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Evaluation\n",
    "optimized_rmse_test = mean_squared_error(y_test, optimized_y_pred1)\n",
    "optimized_r2_test = r2_score(y_test, optimized_y_pred1)\n",
    "print(f\"RMSE for Test Data: \\n{optimized_rmse_test}\")\n",
    "print(f\"R-Squared for Test Data: \\n{optimized_r2_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Optimized Model Cross Validation\n",
    "optimized_score = cross_val_score(optimized_regressor, x_test, y_test, cv = 10)\n",
    "optimized_score_mean = round((score.mean() * 100), 2)\n",
    "optimized_score_std_dev = round((score.std() * 100), 2)\n",
    "print(f\"Cross Validation Mean: {optimized_score_mean}\")\n",
    "print(f\"Cross Validation Standard Deviation: {optimized_score_std_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Optimized Model Feature Importance\n",
    "optimized_imp_features = pd.DataFrame({\"Features\": optimized_model.feature_names_in_, \"Score\": optimized_model.feature_importances_})\n",
    "print(f\"Important Features from Training: \\n{optimized_imp_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2: Air Quality Index creation and prediction\n",
    "Define an Air Quality Index (based on adequate literature) by combining the ground-truth readings of several gases.\n",
    "\n",
    "Then, use ML to predict your Air Quality Index from several raw sensor readings and other columns of interest (obviously without using the ground truth column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Further Data preparation (specific for this task)\n",
    "\n",
    "Creating our Air Quality Index involved usinng the 5 air pollutants and setting a sub-index for each of them based on their level of concentration at that particular hour. Our sub-index ranging from 0 to 4 indicates lower levels as a good level of that air pollutant in the air while 4 indicates very unhealthy or harzardous concentration of that air pollutant. Referencing [WHO ---> Air Quality Guidelines](https://www.who.int/publications/i/item/9789240034433), we successfully created our AQI that tells us the air quality at a particular point in time.\n",
    "\n",
    "As part of our further data preparation steps for this task, we drop columns at three levels:\n",
    "- LEVEL 1: The first drop involes us removing all ground truth labels as they won't be relevant for prediction.\n",
    "- LEVEL 2: Here we drop all missing values across the rows. This is to allow the machine get trained on only the actual readings of sensors and allow for better prediction than employing a strategy for handling missing values.\n",
    "- LEVEL 3: After extracting and creating new features from our date and time columns, we need to drop the categorical features as they aren't relevant anymore.\n",
    "\n",
    "While splitting our data, we follow industry standard and use an 80:20 split of the data towards training and testing. Training the data with 80% of the information in the dataset allows for it to learn properly, patterns in our data which in turn allows for better predictions on the 20% of the data left.\n",
    "\n",
    "Given our classes for prediction are imbalanced, for training our model and to avoid any class imbalance or bias in prediction, we employ the use of the synthetic minority over-sampling technique(SMOTE) from the imblearn library. SMOTE helps to generate synthetic data for the minority classes and helps create an equal number of class width among all classes involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Creating my AQI standard\n",
    "def AQI(dataframe):\n",
    "    data_with_aqi = {\n",
    "        \"CO(GT)\": [],\n",
    "        \"C6H6(GT)\": [],\n",
    "        \"NO2(GT)\": [],\n",
    "        \"NOx(GT)\": [],\n",
    "        \"NMHC(GT)\": [],\n",
    "    }\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        # CO(GT)\n",
    "        if pd.isna(row[\"CO(GT)\"]):\n",
    "            data_with_aqi[\"CO(GT)\"].append(np.nan)\n",
    "        elif 0 <= row[\"CO(GT)\"] <= 4.4:\n",
    "            data_with_aqi[\"CO(GT)\"].append(0)\n",
    "        elif 4.5 <= row[\"CO(GT)\"] <= 9.4:\n",
    "            data_with_aqi[\"CO(GT)\"].append(1)\n",
    "        elif 9.5 <= row[\"CO(GT)\"] <= 14.4:\n",
    "            data_with_aqi[\"CO(GT)\"].append(2)\n",
    "        elif 14.5 <= row[\"CO(GT)\"] <= 24.4:\n",
    "            data_with_aqi[\"CO(GT)\"].append(3)\n",
    "        elif row[\"CO(GT)\"] > 24.4:\n",
    "            data_with_aqi[\"CO(GT)\"].append(4)\n",
    "        else:\n",
    "            data_with_aqi[\"CO(GT)\"].append(np.nan)\n",
    "\n",
    "        # C6H6(GT)\n",
    "        if pd.isna(row[\"C6H6(GT)\"]):\n",
    "            data_with_aqi[\"C6H6(GT)\"].append(np.nan)\n",
    "        elif 0 <= row[\"C6H6(GT)\"] <= 0.54:\n",
    "            data_with_aqi[\"C6H6(GT)\"].append(0)\n",
    "        elif 0.55 <= row[\"C6H6(GT)\"] <= 2.4:\n",
    "            data_with_aqi[\"C6H6(GT)\"].append(1)\n",
    "        elif 2.5 <= row[\"C6H6(GT)\"] <= 4.4:\n",
    "            data_with_aqi[\"C6H6(GT)\"].append(2)\n",
    "        elif 4.5 <= row[\"C6H6(GT)\"] <= 8.4:\n",
    "            data_with_aqi[\"C6H6(GT)\"].append(3)\n",
    "        elif row[\"C6H6(GT)\"] > 8.4:\n",
    "            data_with_aqi[\"C6H6(GT)\"].append(4)\n",
    "        else:\n",
    "            data_with_aqi[\"C6H6(GT)\"].append(np.nan)\n",
    "\n",
    "        # NO2(GT)\n",
    "        if pd.isna(row[\"NO2(GT)\"]):\n",
    "            data_with_aqi[\"NO2(GT)\"].append(np.nan)\n",
    "        elif 0 <= row[\"NO2(GT)\"] <= 25:\n",
    "            data_with_aqi[\"NO2(GT)\"].append(0)\n",
    "        elif 26 <= row[\"NO2(GT)\"] <= 50:\n",
    "            data_with_aqi[\"NO2(GT)\"].append(1)\n",
    "        elif 51 <= row[\"NO2(GT)\"] <= 100:\n",
    "            data_with_aqi[\"NO2(GT)\"].append(2)\n",
    "        elif 101 <= row[\"NO2(GT)\"] <= 200:\n",
    "            data_with_aqi[\"NO2(GT)\"].append(3)\n",
    "        elif row[\"NO2(GT)\"] > 200:\n",
    "            data_with_aqi[\"NO2(GT)\"].append(4)\n",
    "        else:\n",
    "            data_with_aqi[\"NO2(GT)\"].append(np.nan)\n",
    "\n",
    "        # NOx(GT)\n",
    "        if pd.isna(row[\"NOx(GT)\"]):\n",
    "            data_with_aqi[\"NOx(GT)\"].append(np.nan)\n",
    "        elif 0 <= row[\"NOx(GT)\"] <= 30.4:\n",
    "            data_with_aqi[\"NOx(GT)\"].append(0)\n",
    "        elif 30.5 <= row[\"NOx(GT)\"] <= 60.4:\n",
    "            data_with_aqi[\"NOx(GT)\"].append(1)\n",
    "        elif 60.5 <= row[\"NOx(GT)\"] <= 90.4:\n",
    "            data_with_aqi[\"NOx(GT)\"].append(2)\n",
    "        elif 90.5 <= row[\"NOx(GT)\"] <= 120.4:\n",
    "            data_with_aqi[\"NOx(GT)\"].append(3)\n",
    "        elif row[\"NOx(GT)\"] > 120.4:\n",
    "            data_with_aqi[\"NOx(GT)\"].append(4)\n",
    "        else:\n",
    "            data_with_aqi[\"NOx(GT)\"].append(np.nan)\n",
    "\n",
    "        # NMHC(GT)\n",
    "        if pd.isna(row[\"NMHC(GT)\"]):\n",
    "            data_with_aqi[\"NMHC(GT)\"].append(np.nan)\n",
    "        elif 0 <= row[\"NMHC(GT)\"] <= 50:\n",
    "            data_with_aqi[\"NMHC(GT)\"].append(0)\n",
    "        elif 51 <= row[\"NMHC(GT)\"] <= 100:\n",
    "            data_with_aqi[\"NMHC(GT)\"].append(1)\n",
    "        elif 101 <= row[\"NMHC(GT)\"] <= 150:\n",
    "            data_with_aqi[\"NMHC(GT)\"].append(2)\n",
    "        elif 151 <= row[\"NMHC(GT)\"] <= 200:\n",
    "            data_with_aqi[\"NMHC(GT)\"].append(3)\n",
    "        elif row[\"NMHC(GT)\"] > 200:\n",
    "            data_with_aqi[\"NMHC(GT)\"].append(4)\n",
    "        else:\n",
    "            data_with_aqi[\"NMHC(GT)\"].append(np.nan)\n",
    "            \n",
    "    return pd.DataFrame(data_with_aqi)\n",
    "\n",
    "# Assuming 'data' is a DataFrame\n",
    "dataframe = AQI(data_task2)\n",
    "data_task2[\"AQI\"] = np.max(dataframe, axis = 1)\n",
    "print(f\"Hourly AQI: \\n{dataframe}\")\n",
    "print(f\"\\n\\nData with AQI: \\n{data_task2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Dropping columns that won't be useful for AQI prediction\n",
    "data = data_task2.drop([\"CO(GT)\", \"NMHC(GT)\", \"C6H6(GT)\", \"NOx(GT)\", \"NO2(GT)\"], axis = 1)\n",
    "count_null = data.isnull().sum()\n",
    "print(data)\n",
    "print(f\"\\n\\n\\nMissing Values in Columns: \\n{count_null}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Removing all missing values across the rows to improve integrity of prediction after training\n",
    "data = data.dropna()\n",
    "count_null = data.isnull().sum()\n",
    "print(data)\n",
    "print(f\"\\n\\n\\nMissing Values in Columns: \\n{count_null}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Exploratory Data Analysis\n",
    "data.info()\n",
    "data_head = data.head()\n",
    "print(f\"Data Head: \\n\\n{data_head}\")\n",
    "data_tail = data.tail()\n",
    "print(f\"Data Tail: \\n\\n{data_tail}\")\n",
    "data_descriptive_statistic = data.describe()\n",
    "print(f\"Descriptive Statistics: \\n\\n{data_descriptive_statistic}\")\n",
    "data_distinct_count = data.nunique()\n",
    "print(f\"Data Distinct Count: \\n\\n{data_distinct_count}\")\n",
    "data_correlation_matrix = data.corr() \n",
    "print(f\"Correlation Matrix: \\n\\n{data_correlation_matrix}\")\n",
    "data_null_count = data.isnull().sum()\n",
    "print(f\"Missing Values in each Column: \\n\\n{data_null_count}\")\n",
    "data_total_null_count = data.isnull().sum().sum()\n",
    "print(f\"Data Total Missing Values: {data_total_null_count}\")\n",
    "\n",
    "            # ---> Visualization\n",
    "data_histogram = data.hist(bins = 10, figsize = (30, 15), alpha=0.7, color='brown')\n",
    "plt.figure(figsize = (30, 10))\n",
    "data_heatmap = sns.heatmap(data_correlation_matrix, annot = True, cmap = \"coolwarm\")\n",
    "plt.title('Correlation Matrix of Independent Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) Extracting Features from Date and Time to Create New Features\n",
    "data['Datetime'] = data['Date'] + \" \" + data['Time']\n",
    "data.set_index(\"Datetime\", inplace = True)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = data.sort_index(axis = 0)\n",
    "\n",
    "data[\"Year\"] = data.index.year\n",
    "data[\"Month\"] = data.index.month\n",
    "data[\"Day\"] = data.index.day\n",
    "data[\"HourTime\"] = data.index.hour\n",
    "data[\"DayOfWeek\"] = data.index.day_of_week\n",
    "data[\"Quarter\"] = data.index.quarter\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) Dropping additional columns we won't be needing\n",
    "data = data.drop([\"Date\", 'Time'], axis = 1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) Grouping dependent and independent variables for prediction\n",
    "x = data.drop([\"AQI\"], axis = 1) \n",
    "y = data[\"AQI\"]\n",
    "print(f\"Independent Variables: \\n{x}\")\n",
    "print(f\"\\n\\nDependent Variables: \\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (8) Splitting the dataset (80:20)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "print(f\"x_train: \\n{x_train}\")\n",
    "print(f\"y_train: \\n{y_train}\")\n",
    "print(f\"x_test: \\n{x_test}\")\n",
    "print(f\"y_test: \\n{y_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (9) Dealing with an Unbalanced Dataset\n",
    "unbalanced_model_fix = SMOTE()\n",
    "x_train, y_train = unbalanced_model_fix.fit_resample(x_train, y_train)\n",
    "y_train_class_count = y_train.value_counts()\n",
    "print(f\"x_train: \\n{x_train}\")\n",
    "print(f\"y_train: \\n{y_train}\")\n",
    "print(f\"\\n\\nClass Count: \\n{y_train_class_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model definition and training\n",
    "\n",
    "For this classification problem, we employ a Random Forest Classifier. Against all other models tested on this data, the random forest achieves the best balance between generalization and accuracy in prediction. The random forest classifier with baseline parameters without any tuning already does a really good job in predicting our classes.\n",
    "\n",
    "MODEL OPTIMIZATION\n",
    "\n",
    "There was no need creating an optimized model for this task. After multiple simulations and hyperparameter tuning, it was established that the model already performs at the highest level with baseline parameters compared to tuning the parameters. In event that there was an increase in our models prediction, the changes are really low to be considered significant or optimized. Given the situation, working with the model at base parameters was considered best for this project.\n",
    "\n",
    "TRAINING AND PREDICTION\n",
    "\n",
    "While creating our model, we understand the need to focus on the test data as it is the basis of predictions and recommendations. However, we take into account the predictions and happenings while the model was being trained as well. This gives us a clearly image of the stages the model passed through and it's behaviour while training, before prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Base Model Training\n",
    "classifier = RandomForestClassifier(random_state= 0,)\n",
    "model = classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Prediction\n",
    "y_pred = model.predict(x_train)\n",
    "y_pred1 = model.predict(x_test)\n",
    "print(f\"Predictions from Training Data: \\n{y_pred}\")\n",
    "print(f\"Predictions from Test Data: \\n{y_pred1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model evaluation\n",
    "\n",
    "For a classification model evaluation, we will be using the following:\n",
    "- Confusion Matrix ---> This shows us the True Positives, True Negatives, False Positives, and False Negatives across each classes prediction\n",
    "- Classification Report ---> This shows us the percentange summary of the Accuracy, Precision, Recall, F1-Score, Overall Average in prediction from our model. A good place to get clearer on the overall prediction capacity of our model across all metrics.\n",
    "- Accuracy ---> This is the percentage of True Positives our model was able to achieve in prediction.\n",
    "- Precision (Positive Predictive Value) ---> Precision is the ratio of correctly predicted positive observations to the total predicted positives. It assesses the accuracy of positive predictions.\n",
    "- Recall (Sensitivity or True Positive Rate) ---> Recall is the ratio of correctly predicted positive observations to the all observations in actual class. It assesses the model's ability to capture all positive instances.\n",
    "- F1 Score ---> F1 Score is the harmonic mean of Precision and Recall. It provides a balanced assessment of a model's performance.\n",
    "- Cross Validation ---> A technique popular in machine learning that allows you to bootstrap the dataset and run multiple series of simulations of training and testing on the dataset in order to get a clearer picture on the true accuracy in prediction that our model posseses. This step is considered very improtant in building any model.\n",
    "\n",
    "MODEL OPTIMIZATION\n",
    "\n",
    "The model performs best at baseline with default parameters with little to no improvements when the parameters were tuned. Accuracy in prediction as well as other powerful evaluation metrics drop as well when attempting to tilt the model away from the baseline parameters. We tried increasing the number of estimators above the base of 100, and we get either the same or lower reults. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Training Evaluation\n",
    "training_analysis = confusion_matrix(y_train, y_pred)\n",
    "training_class_report = classification_report(y_train, y_pred)\n",
    "training_accuracy = accuracy_score(y_train, y_pred)\n",
    "training_precision = precision_score(y_train, y_pred, average='weighted')\n",
    "training_recall = recall_score(y_train, y_pred, average='weighted')\n",
    "training_f1_score = f1_score(y_train, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Test Evaluation\n",
    "test_analysis = confusion_matrix(y_test, y_pred1)\n",
    "test_class_report = classification_report(y_test, y_pred1)\n",
    "test_accuracy = accuracy_score(y_test, y_pred1)\n",
    "test_precision = precision_score(y_test, y_pred1, average='weighted')\n",
    "test_recall = recall_score(y_test, y_pred1, average='weighted')\n",
    "test_f1_score = f1_score(y_test, y_pred1, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Cross Validation\n",
    "score = cross_val_score(classifier, x_test, y_test, cv = 10)    \n",
    "score_mean = round((score.mean() * 100), 2)\n",
    "score_std_dev = round((score.std() * 100), 2)\n",
    "print(f\"Cross Validation Mean: {score_mean}\")\n",
    "print(f\"Cross Validation Standard Deviation: {score_std_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Feature Importance\n",
    "imp_features = pd.DataFrame({\"Features\": model.feature_names_in_, \"Score\": model.feature_importances_})\n",
    "print(f\"Important Features from Training: \\n{imp_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusions\n",
    "\n",
    "#### TASK 1: \n",
    "We successfully created a model that can predict the concentration of Carbon Monoxide(CO). With an R-Squared above 90%, the model does a good job of modeling the patterns in prediction the CO concentration. One possible suggestion for improvement to consider is a scenario where we bin all the numerical columns according to their levels of concentration and test this against the models predictive capacity. This way we can see if the model picks up other patterns from the data that we missed without data binning. Another will be that quality of data we had to work with. Without any data preparation, we had over 16000 missing values across all rows and columns. Being that the quality of data helps our model better understand and draw patterns between the dependent and independent variable, having a clean dataset would contributed more towards understanding the data.\n",
    "The model's performances on the test dataset with baseline parameters are summarized below:<br>\n",
    "- BASE MODEL  \n",
    "1. RMSE ---> 0.42\n",
    "2. R-Squared ---> 91%\n",
    "3. Cross Validation Mean ---> 88%\n",
    "4. Cross Validation Standard Deviation ---> 2.5\n",
    "\n",
    "- OPTIMIZED MODEL  \n",
    "1. RMSE ---> 0.38\n",
    "2. R-Squared ---> 93%\n",
    "3. Cross Validation Mean ---> 89%\n",
    "4. Cross Validation Standard Deviation ---> 2\n",
    "    \n",
    "\n",
    "#### TASK 2: \n",
    "Our Task 2 model is a classification model that was trained to predict the AQI given the raw sensor readings, time, and some other defined parameters. With a validation mean of 90% and deviation of 1.8, the model does a good job at predicting the Air Quality Standards given the air pollutants in the air. Our prediction shows a 93% for model accuracy, precision, recall, and f1-score, indicating just how good the model is able to detect the patterns and relationship between the features and the AQI it predicts. One possible improvement we could utilize is to use data binning as a way to gain insight on other patterns in our data we may have missed out on without binning. \n",
    "The model's performances on the test dataset with baseline parameters are summarized below:\n",
    "1. Accuracy ---> 93%\n",
    "2. Precision ---> 93%\n",
    "3. Recall ---> 93%\n",
    "4. F1-Score ---> 93%\n",
    "5. Cross Validation Mean ---> 90%\n",
    "6. Cross Validation Standard Deviation ---> 1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "This cell goes to the very bottom of your submitted notebok.\n",
    "You are requried to link the sources and web-links that you have used for various parts of this coursework. \n",
    "\n",
    "Write them sources used in the following format similar to the first examle in the sources list below :\n",
    "\n",
    "    - what you have used them for : web-link\n",
    "\n",
    "Sources:\n",
    "\n",
    "- Implement a recurrent neural network : https://peterroelants.github.io/posts/rnn-implementation-part01/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f4fa1dff685cf8b0e8f68ac358400f6497cf659b705a84b1e00c6e6dfedb2d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
