{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Articial Intelligence and Machine Learning - Coursework 1 - 1st diet\n",
    "## Air quality dataset\n",
    "# Student Name: \n",
    "# Student Email:\n",
    "\n",
    "I confirm that the material contained within the submitted coursework is all my own work unless otherwise stated below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and problem definition\n",
    "Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data ingeston\n",
    "Your text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your python code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the dataset\n",
    "dataset = pd.read_csv(\"AirQuality.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial descriptive statistics\n",
    "print(\"Data Schema\")\n",
    "dataset.info()\n",
    "\n",
    "dataset_descriptive_statistic = dataset.describe()\n",
    "print(f\"\\n\\nData descriptive statistics: \\n{dataset_descriptive_statistic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preparation (common to both tasks)\n",
    "Your text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your python code here\n",
    "# (1) Dropping the empty columns\n",
    "data = dataset.drop([\"Unnamed: 15\", \"Unnamed: 16\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Replacing all values with -200 with np.nan\n",
    "data[data == -200] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Creating the day of the week column\n",
    "data[\"DayOfWeek\"] = pd.to_datetime(data[\"Date\"], dayfirst = True).dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Creating a column indicating Peak Time or Not. \n",
    "# (8AM-12PM and 6-10PM on working days) and (9am-12pm on non-working days).\n",
    "def peak_time(dayofweek):\n",
    "    WorkingDays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
    "    NonWorkingDays = [\"Saturday\", \"Sunday\"]\n",
    "\n",
    "    if (dayofweek[\"DayOfWeek\"] in WorkingDays) and (dayofweek[\"Time\"] >= \"08:00:00\" and dayofweek[\"Time\"] <= \"12:00:00\"):\n",
    "        return 1\n",
    "    elif (dayofweek[\"DayOfWeek\"] in WorkingDays) and (dayofweek[\"Time\"] >= \"18:00:00\" and dayofweek[\"Time\"] <= \"22:00:00\"):\n",
    "        return 1\n",
    "    elif (dayofweek[\"DayOfWeek\"] in NonWorkingDays) and (dayofweek[\"Time\"] >= \"09:00:00\" and dayofweek[\"Time\"] <= \"12:00:00\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    " \n",
    "data[\"PeakTime\"] = data.apply(peak_time, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) Creating a column indicating Valley Road Usage or Not\n",
    "# (Valley road usage is during the central hours of the night (2-6am))\n",
    "def valley_time(time):\n",
    "    if time[\"Time\"] >= \"02:00:00\" and time[\"Time\"] <= \"06:00:00\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data[\"ValleyTime\"] = data.apply(valley_time, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) Visualization\n",
    "def plot_normal_distribution_curve():\n",
    "    data_numerical_columns = data.select_dtypes(\"number\")\n",
    "    count = 0\n",
    "    while count < 13:\n",
    "        data_to_plot = data_numerical_columns.iloc[:, count]\n",
    "        \n",
    "        # Plotting the histogram\n",
    "        plt.figure(figsize = (15, 10))\n",
    "        plt.hist(data_to_plot, bins = 10, density = True, alpha=0.7, rwidth = 8.5)\n",
    "        plt.title(f\"Distribution of {data_to_plot.name}\", pad = 10, size = 25)\n",
    "        plt.xlabel(f\"{data_to_plot.name}\")\n",
    "        \n",
    "        # Plotting the normal distribution\n",
    "        x_values = np.linspace(start = min(data_to_plot), stop = max(data_to_plot), num = 200)\n",
    "        y_values = norm.pdf(x = x_values, loc = data_to_plot.mean(), scale = data_to_plot.std())\n",
    "        \n",
    "        plt.plot(x_values, y_values, color='blue', label='Normal Distribution', linewidth = 1)\n",
    "        plt.show()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "data_histogram = data.hist(bins = 10, figsize = (30, 15), alpha=0.7, color='brown')\n",
    "data_distribution = plot_normal_distribution_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) Extracting Features from Date and Time\n",
    "data[\"Year\"] = pd.to_datetime(data[\"Date\"]).dt.year\n",
    "data[\"Month\"] = pd.to_datetime(data[\"Date\"]).dt.month\n",
    "data[\"Day\"] = pd.to_datetime(data[\"Date\"]).dt.day\n",
    "data[\"HourTime\"] = pd.to_datetime(data[\"Time\"], format = '%H:%M:%S').dt.hour\n",
    "data = pd.get_dummies(data, columns = [\"DayOfWeek\"], dtype = np.int64, drop_first = True, prefix = \"Date\")\n",
    "data = data.drop([\"Date\", \"Time\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (8) Exploratory Data Analysis\n",
    "print(\"Data Schema\")\n",
    "data.info()\n",
    "\n",
    "data_head = data.head()\n",
    "print(f\"\\n\\nTop 5 rows in the Data: \\n{data_head}\")\n",
    "\n",
    "data_tail = data.tail()\n",
    "print(f\"\\n\\nBottom 5 rows in the Data: \\n{data_tail}\")\n",
    "\n",
    "data_descriptive_statistic = data.describe()\n",
    "print(f\"\\n\\nData descriptive statistics: \\n{data_descriptive_statistic}\")\n",
    "\n",
    "data_distinct_count = data.nunique()\n",
    "print(f\"\\n\\nUnique values in columns: \\n{data_distinct_count}\")\n",
    "\n",
    "data_correlation_matrix = data.corr() # Get the correlation matrix of the independent variables\n",
    "print(f\"\\n\\nData correlation matrix: \\n{data_correlation_matrix}\")\n",
    "\n",
    "data_null_count = data.isnull().sum()\n",
    "print(f\"\\n\\nCounting empty rows in each column: \\n{data_null_count}\")\n",
    "\n",
    "data_total_null_count = data.isnull().sum().sum()\n",
    "print(f\"\\n\\nCounting total empty rows in the data: \\n{data_total_null_count}\")\n",
    "\n",
    "            # ---> More Visuals\n",
    "plt.figure(figsize = (30, 10))\n",
    "data_heatmap = sns.heatmap(data_correlation_matrix, annot = True, cmap = \"coolwarm\")\n",
    "plt.title('Correlation Matrix of Independent Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (9) Dropping the NMHC(GT) column\n",
    "data = data.drop([\"NMHC(GT)\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1: CO concentration prediction\n",
    "Predict the CO concentration (in mg/m3) based on, at least, the PT08.S1(CO) raw sensor readings, day of the week and time. \n",
    "\n",
    "Maybe temperature and humidity can play a role as well? \n",
    "\n",
    "Use CO(GT) as the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Further Data preparation (specific for this task)\n",
    "## Data segregation\n",
    "Your text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your Python code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model definition and training\n",
    "Your text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your Python code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model evaluation\n",
    "Your text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2: Air Quality Index creation and prediction\n",
    "Define an Air Quality Index (based on adequate literature) by combining the ground-truth readings of several gases.\n",
    "\n",
    "Then, use ML to predict your Air Quality Index from several raw sensor readings and other columns of interest (obviously without using the ground truth column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Further Data preparation (specific for this task)\n",
    "Your text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model definition and training\n",
    "Your text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model evaluation\n",
    "Your text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusions\n",
    "Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "This cell goes to the very bottom of your submitted notebok.\n",
    "You are requried to link the sources and web-links that you have used for various parts of this coursework. \n",
    "\n",
    "Write them sources used in the following format similar to the first examle in the sources list below :\n",
    "\n",
    "    - what you have used them for : web-link\n",
    "\n",
    "Sources:\n",
    "\n",
    "- Implement a recurrent neural network : https://peterroelants.github.io/posts/rnn-implementation-part01/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f4fa1dff685cf8b0e8f68ac358400f6497cf659b705a84b1e00c6e6dfedb2d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
